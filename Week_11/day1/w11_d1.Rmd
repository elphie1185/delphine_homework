---
title: "R Notebook"
output: html_notebook
---

```{r}
library(readr)
library(tidyverse)
```

```{r}
diamond <- read_csv("diamonds.csv")
diamond_trim <- subset(diamond, select = c("carat", "x", "y", "z"))
```

Load the diamonds.csv data set and undertake an initial exploration of the data. You will find a description of the meanings of the variables on the relevant Kaggle page

We expect the carat of the diamonds to be strong correlated with the physical dimensions x, y and z. Use ggpairs() to investigate correlations between these four variables.
```{r}
library(GGally)
ggpairs(diamond_trim)
```

So, we do find significant correlations. Let’s drop columns x, y and z from the dataset, in preparation to use only carat going forward.
```{r}
diamond_carat <- subset(diamond)[, 1:8]
```


We are interested in developing a regression model for the price of a diamond in terms of the possible predictor variables in the dataset. Use ggpairs() to investigate correlations between price and the predictors (this may take a while to run, don’t worry, make coffee or something).
```{r}
ggpairs(diamond_carat)
```


Perform further ggplot visualisations of any significant correlations you find.
```{r}
diamond_carat %>%
  ggplot(aes(x = carat, y = price)) +
  geom_point()
```

```{r}
diamond_carat %>%
  ggplot(aes(x = cut, y = price)) +
  geom_boxplot()
```

```{r}
diamond_carat %>%
  ggplot(aes(x = color, y = price)) +
  geom_boxplot()
```

```{r}
diamond_carat %>%
  ggplot(aes(x = clarity, y = price)) +
  geom_boxplot()
```


Shortly we may try a regression fit using one or more of the categorical predictors cut, clarity and color, so let’s investigate these predictors: Investigate the factor levels of these predictors. How many dummy variables do you expect for each of them?
```{r}
length(unique(diamond_carat$cut))
length(unique(diamond_carat$color))
length(unique(diamond_carat$clarity))
```

Use the dummy_cols() function in the fastDummies package to generate dummies for these predictors and check the number of dummies in each case.
```{r}
library(fastDummies)
diamond_carat <- dummy_cols(diamond_carat, 
                      select_columns = c("cut", "color", "clarity"), 
                      remove_first_dummy = TRUE)
```

Going forward we’ll let R handle dummy variable creation for categorical predictors in regression fitting (remember lm() will generate the correct numbers of dummy levels automatically, absorbing one of the levels into the intercept as a reference level)
First, we’ll start with simple linear regression. Regress price on carat and check the regression diagnostics.
```{r}
price_carat_model <- lm(price ~ carat, 
                        data = diamond_carat)
summary(price_carat_model)
plot(price_carat_model)
```

Run a regression with one or both of the predictor and response variables log() transformed and recheck the diagnostics. Do you see any improvement?
```{r}
diamond_carat <- diamond_carat %>%
  mutate(price_log = log(price)) %>%
  mutate(carat_log =log(carat))
```

```{r}
log_log_price_carat <- lm(price_log ~ carat_log, 
                          data = diamond_carat)
summary(log_log_price_carat)
plot(log_log_price_carat)
```

```{r}
log_price <- lm(price_log ~ carat, 
                data = diamond_carat)
summary(log_price)
plot(log_price)
```

```{r}
log_carat <- lm(price ~ carat_log, 
                data = diamond_carat)
summary(log_carat)
plot(log_carat)
```

Let’s use log() transformations of both predictor and response. Next, experiment with adding a single categorical predictor into the model. Which categorical predictor is best? [Hint - investigate r2 values]
```{r}
log_log_cut <- lm(price_log ~ carat_log + cut, 
                  data = diamond_carat)
log_log_color <- lm(price_log ~ carat_log + color, 
                  data = diamond_carat)
log_log_clarity <- lm(price_log ~ carat_log + clarity, 
                  data = diamond_carat)
summary(log_log_cut)
summary(log_log_color)
summary(log_log_clarity)
```

Interpret the fitted coefficients for the levels of your chosen categorical predictor. Which level is the reference level? Which level shows the greatest difference in price from the reference level? [Hints - remember we are regressing the log(price) here, and think about what the presence of the log(carat) predictor implies. We’re not expecting a mathematical explanation]
```{r}
unique(diamond_carat$clarity)
```
I1 is the reference level
log_price of diamind is 1.114625 for clarity SI1 than fro reference clarity I1
for a fixed log(carat)

log of something < 1 is negative -> price is lower than reference
log of something > 1 is positive -> increase in price to reference


2 Extension
Try adding an interaction between log(carat) and your chosen categorical predictor. Do you think this interaction term is statistically justified?
```{r}
log_log_caratclarity <- lm(price_log ~ carat_log + clarity + carat_log:clarity, 
                  data = diamond_carat)
summary(log_log_caratclarity)
```
Not justified, 
R_squared still pretty much the same 

Find and plot an appropriate visualisation to show the effect of this interaction
```{r}
library(ggiraphExtra)
ggPredict(log_log_caratclarity)
```
```{r}
coplot(price_log ~ carat_log | clarity, 
       panel = function(x, y, ...){
         points(x, y)
         abline(lm(y ~ x), col = "blue")
       }, 
       overlap = 0.2,
       data = diamond_carat)
```


